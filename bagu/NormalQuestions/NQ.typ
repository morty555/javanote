= 牛客面经
- treeMap和hashMap的区别和底层实现
  - 是否顺序存储：
    - treeMap按key顺序存储
    - hashMap不按顺序存储
  - key可以为null吗？ value呢？
    - treeMap的key不能为null，因为需要排序
    - hashMap可以为null,但是只有一个，因为有覆盖问题
    - value都可以为null，且多个
  - 线程安全
    - 都不是线程安全的
  - 插入，删除时间复杂度
    - treeMap是logn,因为基于红黑树
    - hashMap平均O（1），最坏为O（n），碰撞严重退化成链表，但是链表长度大于8会变成红黑树（jdk1.8以后）
  - 底层实现
    - treeMap的底层实现是红黑树
    - hashMap的底层实现是数组+链表或红黑树 
- 为什么要红黑树不要AVL树
  - AVL树严格平衡二叉树，维护代价高
  - 红黑树平衡策略相对宽松

- 那为什么还要链表
  - 红黑树占用更多内存，红黑树节点大小是普通节点大小的两倍
- 如果链表转为红黑树，还会转回来吗
  - 树节点小于等于6
    - 为什么是6而不是8
      - 避免单节点反复添加删除导致的树化与去树化频繁


- hashmap的底层实现
  - 由数组+链表或红黑树构成
  - put时，先计算key的hash值
  - 然后hash&（n-1）计算索引
  - 如果索引所在位置没有元素则直接放在数组中，如果有
    - 如果key不同，则产生哈希冲突，如果链表的元素小于8或者数组长度小于64，在链表后加入元素
    - 如果key相同，直接替换
  - 如果链表的元素大于8且数组长度>=64,则把链表转换成红黑树
    - 为什么要数组长度大于等于64
      - 避免频繁树化，减小内存占用
      - 数组容量小的时候，扩容自然会减少哈希冲突
      - 容量小更容易扩容，如果不限制数组长度，可能树化完又扩容了
    - 为什么是8 
      - 时间效率和空间占用的平衡
      - 因为红黑树占用内存大，能不用就不用
      - 链表冲突到8的几率很小
  - 扩容机制：
    - 如果数组小于64,链表长度超过 8，优先扩容
    - 如果当前hashmap的容量>容量\*负载因子，则触发扩容 默认容量16,负载因子0.75
      - 为什么是0.75
        - 时间和空间效率的平衡
        - 0.5扩容频繁，查得快
        - 1的话查询效率慢，省空间
    - 扩容容量 ×2，创建新数组，重新计算每个元素的新位置。
    - rehash过程
      - 遍历旧表，对每个桶进行重新分配
        - 如果桶中只有一个节点：直接根据新 hash 重新放入新表。
        - 如果是链表结构：
          - 低位组（(e.hash & oldCap) == 0）留在原索引；
          - 高位组（(e.hash & oldCap) != 0）移动到 index + oldCap。
        - 如果是树结构：
          - 拆分为两棵树或链表，逻辑同上。
  - 为什么容量总是 2 的幂
    - 方便用 (n - 1) & hash 快速计算索引，比直接取余运输快很多
  - 为什么hash 高低位混合
    - 避免哈希分布偏移，提高均匀性
  - 为什么链表树化阈值 8
    - 实际测试中链表长度超过 8 概率极低，树化提升极少但增加复杂度
  - 为什么扩容只移位不重算 hash
    - 节省 CPU 时间，利用位运算特性
  - 为什么负载因子 0.75 
    - 平衡时间与空间效率
  - JDK 8 前后的区别
    - JDK 7，对于桶的哈希冲突问题，采用头插法解决，链表也是纯链表，没有红黑树的扩展，扩容全部重hash
    - JDK8，采用尾插法避免多线程扩容时死循环，采用链表+红黑树的结果，扩容	利用 (hash & oldCap) 判断位置
    - JDK8使key的哈希值的高16位和低16位异或，使得哈希值同时拥有高位和低位的特性，哈希分布更均匀
    - JDK8对元素迁移机制优化
  - hashmap头插法多线程下的死循环问题
    #image("Screenshot_20251017_122249.png")
    - 其实就是头插法下，链表扩容时桶搬迁后是逆序的
    - 但是多线程环境下，A,B线程在搬迁前指向同一个头节点，若A节点时间片用完在休眠，B节点开始搬迁，搬迁后，因为逆序的原因，等到A节点执行时间片的时候，就和新hashmap的存储顺序相反了，结果就会产生死循环
  - 头插法put流程
    - 新元素计算到key所在index有多个元素时
    - 直接将新元素的next指向数组index指向的元素
    - 再让数组index指向新元素就可以了
    - 也就是说，数组[index]存的是链表头节点的索引，每次put新元素时将新元素next指向数组index指向的头节点，改变链表的头节点为新元素，再让数组index指向该元素即可
  - 尾插法put流程
    - 计算到index后先遍历index所在位置的链表直到tail
    - tail.next=新元素
    - 新元素.next=null；
    - 修改新元素为tail
  - hashmap扩容时，顺着数组index所指向的元素一个个迁移，也就是从链表头开始迁移
  - 也就是说 扩容的时候先比较桶各个元素hash & oldCap的值，将为0的放在一个链表，不为0的放在另一个链表，这个过程用的尾插法，当遍历完一个桶所有元素后，将原桶index所在位置指向为0的链表的头节点索引，新桶指向不为0的

  - hashset和hashmap的区别
    - hashset是基于hashmap实现的，不允许重复元素，无序
    - 主要是基于hashmap的key唯一实现的，value为一个常量对象
  - hashmap保证顺序吗
    - 不保证，如果要保证可以用linkedhashmap
    - 排序的话用treemap
  - linkedhashmap使用过吗
    - LRU缓存的实现


- hashmap线程不安全，可以如何优化
  - hashtable，锁全表，效率慢
  - concurrenthashmap，效率高
  - sychronized，效率慢
    
- synchronized原理 
  - synchronized是基于原子性的内部锁机制，是可重入的，因此在一个线程调用synchronized方法的同时在其方法体内部调用该对象另一个synchronized方法，也就是说一个线程得到一个对象锁后再次请求该对象锁，是允许的，这就是synchronized的可重入性。
    - synchronized底层是利用计算机系统mutex Lock实现的。每一个可重入锁都会关联一个线程ID和一个锁状态status。
    - 当一个线程请求方法时，会去检查锁状态。
      - 如果锁状态是0，代表该锁没有被占用，使用CAS操作获取锁，将线程ID替换成自己的线程ID。
      - 如果锁状态不是0，代表有线程在访问该方法。此时，如果线程ID是自己的线程ID，如果是可重入锁，会将status自增1，然后获取到该锁，进而执行相应的方法；如果是非重入锁，就会进入阻塞队列等待。
    - 在释放锁时，
      - 如果是可重入锁的，每一次退出方法，就会将status减1，直至status的值为0，最后释放该锁。
      - 如果非可重入锁的，线程退出方法，直接就会释放该锁。


- 如何在多线程环境下使用HashMap，除了加同步锁方案和ConcurrentHashMap以外还能想到什么
  - 读写锁（ReentrantReadWriteLock）
    - 多读可以并发执行；
    - 写独占
    - 适合读多写少
    - 锁粒度仍然在“整个 Map”层面，性能不如分段锁。
  - 使用 Copy-On-Write 机制
    - 每次写入时复制一份新的 Map，读操作无锁
    - 写时要加sychronized锁
    - 写操作代价大（复制整个 map）；
  - 使用 分段锁（分区锁） 思想
    - 并发度更高，多个分区可同时写；
    - 原理接近 ConcurrentHashMap 1.7 的 “Segment” 分段锁。
  - 消息队列写
  - 使用 ThreadLocal + 合并机制。让每个线程维护自己的本地副本，最后再合并。

- sychronized和reentrantlock
  #image("Screenshot_20251014_201712.png")
  - synchronized是 Java 关键字，由 JVM 原生支持；底层是利用计算机系统mutex，依赖对象头中的monitor
  #image("Screenshot_20251014_201935.png")
  #image("Screenshot_20251014_202135.png")
  - reentrantlock是 普通类，基于 AbstractQueuedSynchronizer（AQS）实现；锁的获取和释放是通过 CAS + FIFO 等待队列（CLH 队列）；
-  介绍一下AQS
  - AQS全称为AbstractQueuedSynchronizer，是Java中的一个抽象类。 AQS是一个用于构建锁、同步器、协作工具类的工具类（框架）。
  - AQS核心思想是，如果被请求的共享资源空闲，那么就将当前请求资源的线程设置为有效的工作线程，将共享资源设置为锁定状态；如果共享资源被占用，就需要一定的阻塞等待唤醒机制来保证锁分配。这个机制主要用的是CLH队列的变体实现的，将暂时获取不到锁的线程加入到队列中。
  - CLH：Craig、Landin and Hagersten队列，是单向链表，AQS中的队列是CLH变体的虚拟双向队列（FIFO），AQS是通过将每条请求共享资源的线程封装成一个节点来实现锁的分配。
  - AQS使用一个Volatile的int类型的成员变量来表示同步状态，通过内置的FIFO队列来完成资源获取的排队工作，通过CAS完成对State值的修改。
  - Sync是AQS的实现。 AQS主要完成的任务：
    - 同步状态（比如说计数器）的原子性管理；
    - 线程的阻塞和解除阻塞；
    - 队列的管理。
  - 在 公平锁 下：进入阻塞队列的线程，只有队首那个会在 state=0 时尝试获取资源。
  - 在 非公平锁 下：队首线程会被唤醒尝试，但新来的线程也可能插队抢到锁。
  -  AQS原理
    - AQS最核心的就是三大部分：
      - 状态：state；
      - 控制线程抢锁和配合的FIFO队列（双向链表）；
      - 期望协作工具类去实现的获取/释放等重要方法（重写）。
    - 状态state
      - 这里state的具体含义，会根据具体实现类的不同而不同：比如在Semapore里，他表示剩余许可证的数量；在CountDownLatch里，它表示还需要倒数的数量；在ReentrantLock中，state用来表示“锁”的占有情况，包括可重入计数，当state的值为0的时候，标识该Lock不被任何线程所占有。
      - state是volatile修饰的，并被并发修改，所以修改state的方法都需要保证线程安全，比如getState、setState以及compareAndSetState操作来读取和更新这个状态。这些方法都依赖于unsafe类。
    - FIFO队列
      - 这个队列用来存放“等待的线程，AQS就是“排队管理器”，当多个线程争用同一把锁时，必须有排队机制将那些没能拿到锁的线程串在一起。当锁释放时，锁管理器就会挑选一个合适的线程来占有这个刚刚释放的锁。
      - AQS会维护一个等待的线程队列，把线程都放到这个队列里，这个队列是双向链表形式。
    - 实现获取/释放等方法
      - 这里的获取和释放方法，是利用AQS的协作工具类里最重要的方法，是由协作类自己去实现的，并且含义各不相同；
      - 获取方法：获取操作会以来state变量，经常会阻塞（比如获取不到锁的时候）。在Semaphore中，获取就是acquire方法，作用是获取一个许可证； 而在CountDownLatch里面，获取就是await方法，作用是等待，直到倒数结束；
      - 释放方法：在Semaphore中，释放就是release方法，作用是释放一个许可证； 在CountDownLatch里面，获取就是countDown方法，作用是将倒数的数减一；
      - 需要每个实现类重写tryAcquire和tryRelease等方法。
    - 当线程尝试加锁或释放锁时，AQS会通过判断 exclusiveOwnerThread 来判断是否“属于自己”：
-  CAS 和 AQS 有什么关系？
  - CAS 是一种乐观锁机制，它包含三个操作数：内存位置（V）、预期值（A）和新值（B）。CAS 操作的逻辑是，如果内存位置 V 的值等于预期值 A，则将其更新为新值 B，否则不做任何操作。整个过程是原子性的，通常由硬件指令支持，如在现代处理器上，cmpxchg 指令可以实现 CAS 操作。
  - AQS 是一个用于构建锁和同步器的框架，许多同步器如 ReentrantLock、Semaphore、CountDownLatch 等都是基于 AQS 构建的。AQS 使用一个 volatile 的整数变量 state 来表示同步状态，通过内置的 FIFO 队列来管理等待线程。它提供了一些基本的操作，如 acquire（获取资源）和 release（释放资源），这些操作会修改 state 的值，并根据 state 的值来判断线程是否可以获取或释放资源。AQS 的 acquire 操作通常会先尝试获取资源，如果失败，线程将被添加到等待队列中，并阻塞等待。release 操作会释放资源，并唤醒等待队列中的线程。
  - CAS 为 AQS 提供原子操作支持：AQS 内部使用 CAS 操作来更新 state 变量，以实现线程安全的状态修改。在 acquire 操作中，当线程尝试获取资源时，会使用 CAS 操作尝试将 state 从一个值更新为另一个值，如果更新失败，说明资源已被占用，线程会进入等待队列。在 release 操作中，当线程释放资源时，也会使用 CAS 操作将 state 恢复到相应的值，以保证状态更新的原子性。
-  如何用 AQS 实现一个可重入的公平锁？
  - 继承 AbstractQueuedSynchronizer：创建一个内部类继承自 AbstractQueuedSynchronizer，重写 tryAcquire、tryRelease、isHeldExclusively 等方法，这些方法将用于实现锁的获取、释放和判断锁是否被当前线程持有。
  - 实现可重入逻辑：在 tryAcquire 方法中，检查当前线程是否已经持有锁，如果是，则增加锁的持有次数（通过 state 变量）；如果不是，尝试使用 CAS操作来获取锁。
  - 在 tryAcquire 方法中，按照队列顺序来获取锁，即先检查等待队列中是否有线程在等待，如果有，当前线程必须进入队列等待，而不是直接竞争锁。
  - 创建锁的外部类：创建一个外部类，内部持有 AbstractQueuedSynchronizer 的子类对象，并提供 lock 和 unlock 方法，这些方法将调用 AbstractQueuedSynchronizer 子类中的方法。


- ==和equals的区别
  - 基本类型不能使用equals
  - ==基本类型比值，引用类型比地址
  - equals默认比地址，重写后不一定
  - String、Integer、Date、集合类中的元素，都重写了 equals()，用来比较内容。所以==和equals可能不一样
- java中BIO,NIO，AIO区别，如何实现，是否有过实际的使用
  - BIO（blocking IO）：就是传统的 java.io 包，它是基于流模型实现的，交互的方式是同步、阻塞方式，也就是说在读入输入流或者输出流时，在读写动作完成之前，线程会一直阻塞在那里，它们之间的调用是可靠的线性顺序。优点是代码比较简单、直观；缺点是 IO 的效率和扩展性很低，容易成为应用性能瓶颈。
  - NIO（non-blocking IO） ：Java 1.4 引入的 java.nio 包，提供了 Channel、Selector、Buffer 等新的抽象，可以构建多路复用的、同步非阻塞 IO 程序，同时提供了更接近操作系统底层高性能的数据操作方式。
  - AIO（Asynchronous IO） ：是 Java 1.7 之后引入的包，是 NIO 的升级版本，提供了异步非堵塞的 IO 操作方式，所以人们叫它 AIO（Asynchronous IO），异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。
  - 底层实现区别
    - BIO：一个请求一个线程，线程调用 socket.read() 时挂起，直到有数据。
    - NIO：基于 I/O 多路复用（select/poll/epoll），一个线程能同时处理多个连接。
    - AIO：基于 操作系统异步 I/O 能力（如 Linux 的 io_uring、Windows 的 IOCP）。
  #image("Screenshot_20250907_235916.png")
  #image("Screenshot_20250908_000213.png")
- 为什么很多大厂只用post发请求
  - URL长度限制
    - GET 请求参数写在 URL 上，不同浏览器、代理、服务器对 URL 长度有限制（常见 2KB ~ 8KB）。
    - 在大厂业务场景中，请求参数可能很复杂（JSON、大量条件、批量 ID、加密串），GET 根本放不下。
    - POST 请求参数放在请求体中，长度没有限制。
  - 安全性
    - GET 请求参数暴露在 URL 上，容易出现在
      - 浏览器地址栏
      - 代理/网关日志
      - 浏览器历史记录
    - 敏感数据（如 token、手机号、订单号）用 POST 放在 body 里更安全。
  - 缓存与幂等性问题
    - HTTP 语义里：
      - GET 应该是安全、幂等的 → 浏览器、中间代理可能会缓存
      - POST 不幂等，默认不缓存
    - 在复杂的分布式系统里，大厂不想依赖中间层“自动缓存”的行为，所以干脆都用 POST，自己做缓存控制。
  - 统一网关与监控
    - 大厂通常有 API 网关、统一鉴权、监控系统。
    - 如果方法不统一（有的用 GET，有的用 DELETE），会增加接入和维护的复杂度。
    - 全部用 POST，统一在 header/body 做参数校验、签名验证、加解密，更加简单。
  - 跨域与复杂请求
    - 浏览器里，GET、POST 的简单请求比较好处理。
    - 但如果需要传自定义 header（如 token、签名），很多情况都会变成 复杂请求 (CORS Preflight)。
    - 大厂干脆约定一律 POST，减少前后端沟通成本。
  - 是一种务实选择，而不是规范选择。

- 长连接和短连接，长连接断开后会发生什么？
  - HTTP是以TCP协议传输的，也就是请求发送前要建立TCP连接
  - 如果每次发送都要建立TCP连接就是短连接，如果建立一次TCP连接可以进行多次请求就是长连接
  - 长连接中断会发生什么？
    - 长连接中断有几种情况
      - TCP连接被动关闭（服务器或客户端主动关闭）
        - 服务器会发送FIN包，TCP进入FIN_WAIT状态
        - 客户端接受到FIN包会进入CLOSE_WAIT状态
        - 之后完成四次挥手
        - 下一次请求时，客户端发现连接已关闭，就会自动重新建立 TCP 连接并重发请求。
      - TCP 连接异常中断（如网络波动、服务器崩溃）
        - 客户端会在写请求时得到 Broken pipe 或 Connection reset 错误；
        - 在在读响应时得到 Connection reset by peer；
        - TCP 状态会变成 CLOSE_WAIT 或 RESET。
        - 当前请求失败；连接池中的该连接会被移除；客户端会尝试重新建立新连接发送请求
      - 请求发送到一半时中断
        - 如果客户端发送请求数据过程中连接断了 请求直接失败；
        - 如果服务器已经收到了部分数据但没收完：服务器可能丢弃请求或返回错误（如 400、408）。
        - 客户端一般不会自动重试非幂等请求（如 POST），以免造成重复提交。 

- 锁升级
  - 一开始某个临界区是无锁状态，如果有线程进入拿锁，就会先判断是否是偏向锁状态，如果不是就加当前线程的偏向锁，也就是把markword中的id设置为当前线程id.如果是偏向锁状态，就会判断markword的id和当前线程是否相同
    - 相同，进入临界区
    - 不相同，产生竞争
      - 若旧线程不再运行，将偏向锁转移
      - 如果此时旧线程还在运行，撤离偏向锁，升级为轻量级锁，轻量级锁采用CAS乐观锁，线程不断CAS自旋，直到拿到锁。如果CAS失败，说明竞争激烈，需要升级为重量级锁
        - 重量级锁未竞争到的锁不再自旋，而是直接挂起，减少cpu消耗


- 线程的创建方式有哪些 
  - 继承 Thread 类
    - 直接继承 Thread 并重写 run() 方法。
    - 调用 start() 方法会自动启动新线程并执行 run()。
    - 缺点： Java 不支持多继承，继承 Thread 后不能再继承其他类。
  - 实现runnable接口
    - 把任务逻辑和线程对象分离，更符合面向对象思想。
    - 可以共享同一个 Runnable 对象，实现多线程共享资源。
  - 使用 匿名内部类 或 Lambda 表达式
    - 实际底层还是实现了 Runnable。
  - 实现 Callable 接口 + FutureTask
    - 如果你需要 线程有返回值 或 抛出异常，必须使用 Callable。


- 线程池原理及拒绝策略
  - 介绍一下线程池的原理
    - 线程池设定了核心线程数，和最大线程数，如果任务进来
      - 核心线程没有满，就新建核心线程执行任务
      - 如果核心线程满了，则放入阻塞队列，阻塞队列中的任务等待核心线程空闲后执行
        - 如果阻塞队列满了，如果没有达到最大线程数，就要新建线程执行新提交的任务（注意不是队列里的）
          - 如果达到了最大线程数，就要执行拒绝策略
  - 线程池的参数
    - corePoolSize：线程池核心线程数量。默认情况下，线程池中线程的数量如果 <= corePoolSize，那么即使这些线程处于空闲状态，那也不会被销毁。
    - maximumPoolSize：限制了线程池能创建的最大线程总数（包括核心线程和非核心线程），当 corePoolSize 已满 并且 尝试将新任务加入阻塞队列失败（即队列已满）并且 当前线程数 < maximumPoolSize，就会创建新线程执行此任务，但是当 corePoolSize 满 并且 队列满 并且 线程数已达 maximumPoolSize 并且 又有新任务提交时，就会触发拒绝策略。
    - keepAliveTime：当线程池中线程的数量大于corePoolSize，并且某个线程的空闲时间超过了keepAliveTime，那么这个线程就会被销毁。
    - unit：就是keepAliveTime时间的单位。
    - workQueue：工作队列。当没有空闲的线程执行新任务时，该任务就会被放入工作队列中，等待执行。
    - threadFactory：线程工厂。可以用来给线程取名字等等
    - handler：拒绝策略。当一个新任务交给线程池，如果此时线程池中有空闲的线程，就会直接执行，如果没有空闲的线程，就会将该任务加入到阻塞队列中，如果阻塞队列满了，就会创建一个新线程，从阻塞队列头部取出一个任务来执行，并将新任务加入到阻塞队列末尾。如果当前线程池中线程的数量等于maximumPoolSize，就不会创建新线程，就会去执行拒绝策略
  - 线程池工作队列满了有哪些拒绝策略？
    - CallerRunsPolicy，使用线程池的调用者所在的线程去执行被拒绝的任务，除非线程池被停止或者线程池的任务队列已有空缺。
    - AbortPolicy，直接抛出一个任务被线程池拒绝的异常。
    - DiscardPolicy,不做任何处理，静默拒绝提交的任务
    - DiscardOldestPolicy，抛弃最老的任务，然后执行该任务。
    - 自定义拒绝策略
  - 阻塞队列有哪些
    - 有界队列
      - 基于数组实现
      - 有固定大小
      - 可选 公平/非公平 锁；
    -  LinkedBlockingQueue
      - 基于 链表 实现；
      - 默认容量为 Integer.MAX_VALUE（几乎无界）
      - 可通过构造函数指定容量。
    - PriorityBlockingQueue
      - 无界优先级队列
      - 元素按 自然顺序 或 Comparator 排序；
      - 不保证 FIFO 顺序。
      - 内部使用 堆结构（PriorityQueue）。
    - DelayQueue
      - 基于 PriorityQueue 实现；
      - 队列中的元素必须实现 Delayed 接口；
      - 只有到期（延时结束）的元素才能被取出。
    - SynchronousQueue
      - 容量为 0 的队列；
      - 每次 put() 必须等待一个 take()；
      - 不能缓存元素。
    -  LinkedBlockingDeque
      - 双向链表实现的 双端阻塞队列；
      - 支持在队头、队尾插入/移除；
    - LinkedTransferQueue
      - 无界的 高性能队列；
      - 支持 直接传递（transfer） 给消费者；
      - 性能优于 LinkedBlockingQueue；
      - 内部基于 CAS + 链表；
      - 常用于高并发场景。

- 如何创建线程池
  - 使用 Executors 工具类
    ```java 
    ExecutorService pool = Executors.newFixedThreadPool(10);
    ```
    - Executors 默认使用的队列往往是 无界队列容易导致溢出
  - 手动创建 ThreadPoolExecutor
    ```java
      ThreadPoolExecutor pool = new ThreadPoolExecutor(
      corePoolSize,        // 核心线程数
      maximumPoolSize,     // 最大线程数
      keepAliveTime,       // 非核心线程空闲存活时间
      TimeUnit.SECONDS,    // 时间单位
      new ArrayBlockingQueue<>(100), // 阻塞队列
      Executors.defaultThreadFactory(), // 线程工厂
      new ThreadPoolExecutor.AbortPolicy() // 拒绝策略
  );
 
    ```
  - CPU 密集型任务 	核心线程数 = CPU 核数 + 1
  - IO 密集型任务 	核心线程数 = 2 × CPU 核数


- JVM内存模型
  - JVM的内存结构主要分为以下几个部分：
    - 程序计数器：可以看作是当前线程所执行的字节码的行号指示器，用于存储当前线程正在执行的 Java 方法的 JVM 指令地址。如果线程执行的是 Native 方法，计数器值为 null。是唯一一个在 Java 虚拟机规范中没有规定任何 OutOfMemoryError 情况的区域，生命周期与线程相同。
    - Java 虚拟机栈：每个线程都有自己独立的 Java 虚拟机栈，生命周期与线程相同。每个方法在执行时都会创建一个栈帧，用于存储局部变量表、操作数栈、动态链接、方法出口等信息。可能会抛出 StackOverflowError 和 OutOfMemoryError 异常。
    - 本地方法栈：与 Java 虚拟机栈类似，主要为虚拟机使用到的 Native 方法服务，在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。本地方法执行时也会创建栈帧，同样可能出现 StackOverflowError 和 OutOfMemoryError 两种错误。
    - Java 堆：是 JVM 中最大的一块内存区域，被所有线程共享，在虚拟机启动时创建，用于存放对象实例。从内存回收角度，堆被划分为新生代和老年代，新生代又分为 Eden 区和两个 Survivor 区（From Survivor 和 To Survivor）。如果在堆中没有内存完成实例分配，并且堆也无法扩展时会抛出 OutOfMemoryError 异常。
    - 方法区（元空间）：在 JDK 1.8 及以后的版本中，方法区被元空间取代，使用本地内存。用于存储已被虚拟机加载的类信息、常量、静态变量等数据。虽然方法区被描述为堆的逻辑部分，但有 “非堆” 的别名。方法区可以选择不实现垃圾收集，内存不足时会抛出 OutOfMemoryError 异常。
    - 运行时常量池：是方法区的一部分，用于存放编译期生成的各种字面量和符号引用，具有动态性，运行时也可将新的常量放入池中。当无法申请到足够内存时，会抛出 OutOfMemoryError 异常。
    - 直接内存：不属于 JVM 运行时数据区的一部分，通过 NIO 类引入，是一种堆外内存，可以显著提高 I/O 性能。直接内存的使用受到本机总内存的限制，若分配不当，可能导致 OutOfMemoryError 异常。

- 方法区中方法执行过程
  - 解析方法调用：JVM会根据方法的符号引用找到实际的方法地址（如果之前没有解析过的话）。
  - 栈帧创建：在调用一个方法前，JVM会在当前线程的Java虚拟机栈中为该方法分配一个新的栈帧，用于存储局部变量表、操作数栈、动态链接、方法出口等信息。
  - 执行方法：执行方法内的字节码指令，涉及的操作可能包括局部变量的读写、操作数栈的操作、跳转控制、对象创建、方法调用等。
  - 返回处理：方法执行完毕后，可能会返回一个结果给调用者，并清理当前栈帧，恢复调用者的执行环境。
  #image("Screenshot_20251018_112259.png")



- JMM
  - JMM 是 线程之间如何共享变量、保证可见性与有序性 的抽象模型。它属于“并发语义”层面的内存模型，不等同于上面的运行时内存结构。
  - 主要关注三件事：
    - 可见性：一个线程对共享变量的修改，能否被其他线程看到。
    - 原子性：一个操作是否是不可分割的。
    - 有序性：程序执行的顺序是否与代码顺序一致。
  - JMM 定义了 主内存（Main Memory） 和 工作内存（Working Memory）：
    - 主内存：存放所有共享变量。
    - 工作内存：每个线程都有一份主内存的拷贝（缓存），线程对变量的操作必须先在工作内存中进行，然后同步回主内存。

- 用户线程与内核线程
  - 用户线程
    - 由用户空间的线程库（如 Java 的线程库、POSIX pthread 等）管理，操作系统内核对此 并不感知。
    - 用户线程的创建、调度、销毁等都是由 用户态代码 完成的，不需要系统调用。
    - 用 Java 创建 new Thread() 或 Python 的 threading.Thread()，实际上对应的是用户线程层面的抽象。
  - 内核线程（Kernel Thread, KLT）
    - 由操作系统内核直接管理。
    - 内核会为每个内核线程分配 内核栈、线程控制块（TCB），并参与调度（即由操作系统调度器负责运行）。
    - 创建、销毁、上下文切换都要经过 系统调用。
  - 用户线程和内核线程的映射模型
    - 1:1 模型 	每个用户线程对应一个内核线程。
    - N:1 模型  多个用户线程映射到同一个内核线程，由用户线程库调度。
    - M:N 模型  M 个用户线程映射到 N 个内核线程（混合模型）。
  - JAVA是哪种模型
    - Java 的线程在现代 JVM（HotSpot）中采用 1:1 模型：
    - 优点：多核利用率高；
    - 缺点：线程数量受系统资源限制（通常几千~上万线程后就不行了）。

- 类加载流程
  - 类从被加载到虚拟机内存开始，到卸载出内存为止，它的整个生命周期包括以下 7 个阶段：
    - 加载：通过类的全限定名（包名 + 类名），获取到该类的.class文件的二进制字节流，将二进制字节流所代表的静态存储结构，转化为方法区运行时的数据结构，在内存中生成一个代表该类的Java.lang.Class对象，作为方法区这个类的各种数据的访问入口
    - 连接：验证、准备、解析 3 个阶段统称为连接。 
      - 验证：确保class文件中的字节流包含的信息，符合当前虚拟机的要求，保证这个被加载的class类的正确性，不会危害到虚拟机的安全。验证阶段大致会完成以下四个阶段的检验动作：文件格式校验、元数据验证、字节码验证、符号引用验证
      - 准备：为类中的静态字段分配内存，并设置默认的初始值，比如int类型初始值是0。被final修饰的static字段不会设置，因为final在编译的时候就分配了
      - 解析阶段是虚拟机将常量池的「符号引用」直接替换为「直接引用」的过程。符号引用是以一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用的时候可以无歧义地定位到目标即可。直接引用可以是直接指向目标的指针、相对偏移量或是一个能间接定位到目标的句柄，直接引用是和虚拟机实现的内存布局相关的。如果有了直接引用， 那引用的目标必定已经存在在内存中了。

    - 初始化：初始化是整个类加载过程的最后一个阶段，初始化阶段简单来说就是执行类的构造器方法（() ），执行类中定义的 静态代码块 和 静态变量的赋值语句。，要注意的是这里的构造器方法()并不是开发者写的，而是编译器自动生成的。
    - 使用：使用类或者创建对象
    - 卸载：一个类要被JVM卸载，条件非常苛刻，需要同时满足以下三点： 
      - 该类所有的实例都已经被回收：这是最显而易见的前提。如果堆中还存在这个类的任何一个实例对象，那么定义这个对象的Class对象肯定不能被卸载。
      - 加载该类的ClassLoader已经被回收：这是最关键也是最难满足的条件。类与其加载器是双向绑定的共生关系。一个类由哪个类加载器加载，这个信息是存储在Class对象里的。要卸载一个类，必须先卸载加载它的类加载器。
      - 类对应的Java.lang.Class对象没有任何地方被引用：不能在任何地方通过反射（如静态字段、全局变量）、静态变量、JNI等途径引用到这个Class对象。一旦这个Class对象还存在强引用，GC就不会回收它，那么这个类也就不会被卸载。

- 类加载器与双亲委派机制
  - 类加载器有哪些？
    - 启动类加载器（Bootstrap Class Loader）：这是最顶层的类加载器，负责加载Java的核心库（如位于jre/lib/rt.jar中的类），它是用C++编写的，是JVM的一部分。启动类加载器无法被Java程序直接引用。
    - 扩展类加载器（Extension Class Loader）：它是Java语言实现的，继承自ClassLoader类，负责加载Java扩展目录（jre/lib/ext或由系统变量Java.ext.dirs指定的目录）下的jar包和类库。扩展类加载器由启动类加载器加载，并且父加载器就是启动类加载器。
    - 系统类加载器（System Class Loader）/ 应用程序类加载器（Application Class Loader）：这也是Java语言实现的，负责加载用户类路径（ClassPath）上的指定类库，是我们平时编写Java程序时默认使用的类加载器。系统类加载器的父加载器是扩展类加载器。它可以通过ClassLoader.getSystemClassLoader()方法获取到。
    - 自定义类加载器（Custom Class Loader）：开发者可以根据需求定制类的加载方式，比如从网络加载class文件、数据库、甚至是加密的文件中加载类等。自定义类加载器可以用来扩展Java应用程序的灵活性和安全性，是Java动态性的一个重要体现。
    - 这些类加载器之间的关系形成了双亲委派模型，其核心思想是当一个类加载器收到类加载的请求时，首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一层次的类加载器都是如此，因此所有的加载请求最终都应该传送到顶层的启动类加载器中。
    - 只有当父加载器反馈自己无法完成这个加载请求（它的搜索范围中没有找到所需的类）时，子加载器才会尝试自己去加载。
  - JDK 9 是从“包级开发”迈向“模块化系统开发”的分水岭。它用模块取代了扩展机制，优化了性能与结构，从此 JDK 不再是一个“巨大的 rt.jar”，而是由多个可独立加载的模块组成的系统。


  - 双亲委派模型的作用  
    - 保证类的唯一性：通过委托机制，确保了所有加载请求都会传递到启动类加载器，即使 ExtClassLoader 无法加载目标类，它仍然会先把请求传递给父加载器（Bootstrap）。并且每个类在同一个 ClassLoader 中只会被加载一次，避免了不同类加载器重复加载相同类的情况，保证了Java核心类库的统一性，也防止了用户自定义类覆盖核心类库的可能。
    - 保证安全性：由于Java核心库被启动类加载器加载，而启动类加载器只加载信任的类路径中的类，这样可以防止不可信的类假冒核心类，增强了系统的安全性。防止用户自定义与核心类（如 java.lang.String）同名的类，污染 JVM 命名空间例如，恶意代码无法自定义一个Java.lang.System类并加载到JVM中，因为这个请求会被委托给启动类加载器，而启动类加载器只会加载标准的Java库中的类。
    - 支持隔离和层次划分：双亲委派模型支持不同层次的类加载器服务于不同的类加载需求，如应用程序类加载器加载用户代码，扩展类加载器加载扩展框架，启动类加载器加载核心库。这种层次化的划分有助于实现沙箱安全机制，保证了各个层级类加载器的职责清晰，也便于维护和扩展。
    - 简化了加载流程：简化了加载流程：通过委派，大部分类能够被正确的类加载器加载，减少了每个加载器需要处理的类的数量，简化了类的加载过程，每个加载器只负责自己范围内的类，减少复杂性，提高效率
    - 为什么说是「大部分类」而不是「全部类」
    #image("Screenshot_20251018_155925.png")
   
- 垃圾回收算法
  - 标记清除 
    - old 
    - 简单
    - 碎片多
    - 从GCroot开始，标记所有可达的对象， 清除所有没有被标记的对象
  - 标记整理 
    - old 
    - 移动成本高
    - 避免碎片
    - 在标记清除上改进，标记存活对象，将存活对象往一端移动
  - 复制算法 
    - young 
    - 无碎片，效率高 
    - 空间浪费一半 
    - 将内存分为两块（From 和 To）；每次只使用其中一块；回收时，把存活对象复制到另一块，然后清空旧区。
  - 分代收集 
    - 高性能 
    - 实现复杂
    - 划分为新生代，老年代，元空间
    - 新生代minorGC，老年代fullgc或majorgc

- 垃圾收集器
  - Serial收集器（复制算法): 新生代单线程收集器，标记和清理都是单线程，优点是简单高效；
  - ParNew收集器 (复制算法): 新生代收并行集器，实际上是Serial收集器的多线程版本，在多核CPU环境下有着比Serial更好的表现；
    - 注重响应速度，降低停顿时间，适合交互式应用
  - Parallel Scavenge收集器 (复制算法): 新生代并行收集器，追求高吞吐量，高效利用 CPU。吞吐量 = 用户线程时间/(用户线程时间+GC线程时间)，高吞吐量可以高效率的利用CPU时间，尽快完成程序的运算任务，适合后台应用等对交互相应要求不高的场景；
    - 注重高吞吐量，但是停顿长，适合计算密集型任务
  - Serial Old收集器 (标记-整理算法): 老年代单线程收集器，Serial收集器的老年代版本；
  - Parallel Old收集器 (标记-整理算法)： 老年代并行收集器，吞吐量优先，Parallel Scavenge收集器的老年代版本；
  - CMS(Concurrent Mark Sweep)收集器（标记-清除算法）： 老年代并行收集器，以获取最短回收停顿时间为目标的收集器，具有高并发、低停顿的特点，追求最短GC回收停顿时间。
  - G1(Garbage First)收集器 (标记-整理算法)： Java堆并行收集器，G1收集器是JDK1.7提供的一个新收集器，G1收集器基于“标记-整理”算法实现，也就是说不会产生内存碎片。此外，G1收集器不同于之前的收集器的一个重要特点是：G1回收的范围是整个Java堆(包括新生代，老年代)，而前六种收集器回收的范围仅限于新生代或老年代
   



- 介绍threadlocal原理
  - threadlocal内部有一个threadlocalmap
    - Entry
      - key 是一个 弱引用的 ThreadLocal 对象。
      - value 是当前线程保存的具体值。
    - table：存储所有 Entry 的数组。
    - threshold：阈值（扩容时用）。
    - size：当前存储的数量。
  - key为什么是弱引用
    - 如果 ThreadLocal 对象被外部没有强引用持有，GC 就可以回收它；
    - 但是 ThreadLocalMap 里的 Entry 仍然会存在，key 会变成 null（被回收）；
    - 若不清理，会导致 value 无法访问但还在内存中 → 内存泄漏。
  - 因此ThreadLocalMap 有自清理机制：
    - 在每次 set() 或 get() 时，ThreadLocalMap 会自动清理那些key == null 的 Entry；也就是 ThreadLocal 已经被 GC 掉的项。

- redis怎么从成本或者使用上优化 
  - 减少内存占用
    - 选对数据结构
    - 合理设置过期时间 TTL
    - 压缩与编码优化
    - 删除冗余数据
  - 降低硬件成本
    - 使用内存压缩型实例
    - 冷热分离。热数据放 Redis；冷数据放 MySQL / ElasticSearch；
  - 减少副本节点数量
  - 持久化优化
    - 不建议高频 RDB，会大量占用磁盘 I/O。
  - 降低访问压力
    - 本地缓存 + Redis 缓存双层结构
  - 批量操作（Pipeline）把多个命令合并成一次请求，减少网络往返
  - 分布式限流 / 防击穿
    - 利用 Lua 脚本原子执行；
    - 热 key 使用本地缓存或互斥锁。
  - 集群与分片优化
    - 使用 Redis Cluster 分片扩展；
    - 控制 key 的 哈希标签（HashTag） 保持相关性；

- redis的主从同步是怎么实现的
  - 从节点发起复制请求
    - 连接主节点。
    - 发送 PING 以测试主从之间网络是否通畅。
    - 进行认证（如果主节点设置了 requirepass）。
    - 发送 PSYNC 命令请求同步。
  - 主从协商同步方式
    - 主节点根据 PSYNC 命令决定同步模式：
      - 全量同步（Full Resync）
      - 部分同步（Partial Resync）
  - 全量同步过程
    - 全量同步是最初始的、最耗时的一次同步
    - 主节点创建 RDB 快照，主节点执行 BGSAVE，在后台生成一个 RDB 文件。
    - RDB 传输，RDB 文件生成完毕后，主节点将文件通过 socket 发送给从节点。从节点接收 RDB 文件后：
      - 清空本地旧数据；
      - 加载新的 RDB 到内存。
    - 命令传播缓冲
      - 在主节点生成 RDB 期间，如果有新的写命令进来，主机会将这些命令写入一个缓冲区（称为 Replication Buffer 或 backlog buffer）。当 RDB 传输完成后，主节点会将缓冲区的写操作再发给从节点，以保持最终一致。
  - 增量同步
    - 当主从断开后重新连接，如果主节点仍然保留了从节点上次同步的复制偏移量（offset） 和 runid，则可以进行增量同步。
    - 从节点发送 PSYNC <runid> <offset>。
    - 主节点检查 backlog buffer 是否仍然有该 offset 之后的数据：
      - 有：返回 CONTINUE，仅发送增量命令。
      - 无：返回 FULLRESYNC，重新全量同步。
      - 这就是 Redis 2.8+ 后改进的 部分重同步机制
  - 主从完成一次全量或部分同步后，会进入命令传播阶段：
    - 主节点每执行一个写命令（如 SET、DEL、LPUSH 等），都会将该命令传播给所有从节点；
    - 从节点按顺序执行这些命令；

  - 部分同步的目标
    - 在早期（Redis 2.8 以前），主从断开后，从节点只能重新全量同步：
    - 部分同步让主从短暂断开后，只同步“断开期间”丢失的那一小部分数据，而不是重新同步全部数据。

  - 如果同步后有增量同步 那不是只要一次全量同步就好了吗
    - 只要主从完成一次全量同步，之后只要连接不断、网络稳定，主从之间确实就会一直通过 增量同步（命令传播） 保持一致，不需要再做任何全量同步。
    - 因为在真实环境中，有几种情况会打破这种理想状态，导致无法继续增量同步，只能重新全量。
      - 主从网络断开，错过了太多数据
      - 主节点重启了（runid 改变）
      - 从节点第一次加入复制集
      - 主从数据不一致（人为修改或过期）
  
  - 主从同步的间隔
    - Redis 主从同步不是定时拉取的
      - Redis 的主从复制是事件驱动 + 异步推送机制，
      - 主节点执行写命令 → 主节点立即推送给从节点。
    - 但为了保证主从状态健康，它们确实有“定期通信”
      - 主从之间确实会定期发送一些心跳包、offset 信息等
        - 检测连接是否正常
        - 判断延迟和同步进度
        - 确认是否需要补发数据
      #image("Screenshot_20251015_113321.png")
    - 从节点会每秒向主节点发送一次 REPLCONF ACK <offset> 命令：主节点会保存每个从节点的最新 offset，用来判断每个从节点的复制延迟与健康状态。
    - 主节点保存所有从节点的复制状态信息，主节点通过这些信息：
      - 知道哪些从节点落后；
      - 触发部分同步（PSYNC）时判断是否能补齐；
      - 在 Sentinel / Cluster 模式下评估从节点是否可被提升为主。
    - backlog是个固定长度的环形缓冲区（circular buffer），用于保存最近写入命令的数据流。
      - 当从节点断开后又重连时，如果它上次同步的位置还在 backlog 范围内，就可以执行部分重同步（增量同步）；
      - 否则，只能执行全量同步。
        - 因为是环形，当backlog写满后会覆盖原来的数据，如果从节点长时间没有与主节点进行部分同步，backlog一直增加就会造成覆盖
    #image("Screenshot_20251015_113641.png")

- Redis的大key问题 
  -  Redis的大Key问题是什么？
    - Redis大key问题指的是某个key对应的value值所占的内存空间比较大，导致Redis的性能下降、内存不足、数据不均衡以及主从同步延迟等问题。
    - 到底多大的数据量才算是大key？
    - 没有固定的判别标准，通常认为字符串类型的key对应的value值占用空间大于1M，或者集合类型的k元素数量超过1万个，就算是大key。
    - Redis大key问题的定义及评判准则并非一成不变，而应根据Redis的实际运用以及业务需求来综合评估。
    - 例如，在高并发且低延迟的场景中，仅10kb可能就已构成大key；然而在低并发、高容量的环境下，大key的界限可能在100kb。因此，在设计与运用Redis时，要依据业务需求与性能指标来确立合理的大key阈值。
  - 大Key问题的缺点？
    - 内存占用过高。大Key占用过多的内存空间，可能导致可用内存不足，从而触发内存淘汰策略。在极端情况下，可能导致内存耗尽，Redis实例崩溃，影响系统的稳定性。
    - 性能下降。大Key会占用大量内存空间，导致内存碎片增加，进而影响Redis的性能。对于大Key的操作，如读取、写入、删除等，都会消耗更多的CPU时间和内存资源，进一步降低系统性能。
    - 阻塞其他操作。某些对大Key的操作可能会导致Redis实例阻塞。例如，使用DEL命令删除一个大Key时，可能会导致Redis实例在一段时间内无法响应其他客户端请求，从而影响系统的响应时间和吞吐量。
    - 网络拥塞。每次获取大key产生的网络流量较大，可能造成机器或局域网的带宽被打满，同时波及其他服务。例如：一个大key占用空间是1MB，每秒访问1000次，就有1000MB的流量。
    - 主从同步延迟。当Redis实例配置了主从同步时，大Key可能导致主从同步延迟。由于大Key占用较多内存，同步过程中需要传输大量数据，这会导致主从之间的网络传输延迟增加，进而影响数据一致性。
    - 数据倾斜。在Redis集群模式中，某个数据分片的内存使用率远超其他数据分片，无法使数据分片的内存资源达到均衡。另外也可能造成Redis内存达到maxmemory参数定义的上限导致重要的key被逐出，甚至引发内存溢出。

  - Redis大key如何解决？
    - 对大Key进行拆分。例如将含有数万成员的一个HASH Key拆分为多个HASH Key，并确保每个Key的成员数量在合理范围。在Redis集群架构中，拆分大Key能对数据分片间的内存平衡起到显著作用。
    - 对大Key进行清理。将不适用Redis能力的数据存至其它存储，并在Redis中删除此类数据。注意，要使用异步删除。
    - 监控Redis的内存水位。可以通过监控系统设置合理的Redis内存报警阈值进行提醒，例如Redis内存使用率超过70%、Redis的内存在1小时内增长率超过20%等。
    - 对过期数据进行定期清。堆积大量过期数据会造成大Key的产生，例如在HASH数据类型中以增量的形式不断写入大量数据而忽略了数据的时效性。可以通过定时任务的方式对失效数据进行清理。

- redis的过期删除和缓存淘汰
  - 过期删除策略和内存淘汰策略有什么区别？
    - 区别：
      - 内存淘汰策略是在内存满了的时候，redis 会触发内存淘汰策略，来淘汰一些不必要的内存资源，以腾出空间，来保存新的内容
      - 过期键删除策略是将已过期的键值对进行删除，Redis 采用的删除策略是惰性删除+定期删除。
  - 介绍一下Redis 内存淘汰策略
    - 在 32 位操作系统中，maxmemory 的默认值是 3G，因为 32 位的机器最大只支持 4GB 的内存，而系统本身就需要一定的内存资源来支持运行，所以 32 位操作系统限制最大 3 GB 的可用内存是非常合理的，这样可以避免因为内存不足而导致 Redis 实例崩溃。
    - Redis 内存淘汰策略共有八种，这八种策略大体分为「不进行数据淘汰」和「进行数据淘汰」两类策略。
    - 不进行数据淘汰的策略：
      - noeviction（Redis3.0之后，默认的内存淘汰策略） ：它表示当运行内存超过最大设置内存时，不淘汰任何数据，这时如果有新的数据写入，会报错通知禁止写入，不淘汰任何数据，但是如果没用数据写入的话，只是单纯的查询或者删除操作的话，还是可以正常工作。
    - 进行数据淘汰的策略：
      - 针对「进行数据淘汰」这一类策略，又可以细分为「在设置了过期时间的数据中进行淘汰」和「在所有数据范围内进行淘汰」这两类策略。
        - 在设置了过期时间的数据中进行淘汰：
          - volatile-random：随机淘汰设置了过期时间的任意键值；
          - volatile-ttl：优先淘汰更早过期的键值。
          - volatile-lru（Redis3.0 之前，默认的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最久未使用的键值；
          - volatile-lfu（Redis 4.0 后新增的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最少使用的键值；
        - 在所有数据范围内进行淘汰：
          - allkeys-random：随机淘汰任意键值;
          - allkeys-lru：淘汰整个键值中最久未使用的键值；
          - allkeys-lfu（Redis 4.0 后新增的内存淘汰策略）：淘汰整个键值中最少使用的键值。

  - 介绍一下Redis过期删除策略
    - Redis 选择「惰性删除+定期删除」这两种策略配和使用，以求在合理使用 CPU 时间和避免内存浪费之间取得平衡。
    - Redis 的惰性删除策略由 db.c 文件中的 expireIfNeeded 函数实现
    - Redis 在访问或者修改 key 之前，都会调用 expireIfNeeded 函数对其进行检查，检查 key 是否过期：
      - 如果过期，则删除该 key，至于选择异步删除，还是选择同步删除，根据 lazyfree_lazy_expire 参数配置决定（Redis 4.0版本开始提供参数），然后返回 null 客户端；
      - 如果没有过期，不做任何处理，然后返回正常的键值对给客户端；
    - Redis 的定期删除是每隔一段时间「随机」从数据库中取出一定数量的 key 进行检查，并删除其中的过期key。
      - 这个间隔检查的时间是多长呢？
        - 在 Redis 中，默认每秒进行 10 次过期检查一次数据库，此配置可通过 Redis 的配置文件 redis.conf 进行配置，配置键为 hz 它的默认值是 hz 10。特别强调下，每次检查数据库并不是遍历过期字典中的所有 key，而是从数据库中随机抽取一定数量的 key 进行过期检查。
      - 随机抽查的数量是多少呢？
        - 我查了下源码，定期删除的实现在 expire.c 文件下的 activeExpireCycle 函数中，其中随机抽查的数量由 ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP 定义的，它是写死在代码中的，数值是 20。也就是说，数据库每轮抽查时，会随机选择 20 个 key 判断是否过期。接下来，详细说说 Redis 的定期删除的流程：
          - 从过期字典中随机抽取 20 个 key；
          - 检查这 20 个 key 是否过期，并删除已过期的 key；
          - 如果本轮检查的已过期 key 的数量，超过 5 个（20/4），也就是「已过期 key 的数量」占比「随机抽取 key 的数量」大于 25%，则继续重复步骤 1；如果已过期的 key 比例小于 25%，则停止继续删除过期 key，然后等待下一轮再检查。
      - 可以看到，定期删除是一个循环的流程。那 Redis 为了保证定期删除不会出现循环过度，导致线程卡死现象，为此增加了定期删除循环流程的时间上限，默认不会超过 25ms。
  - Redis的缓存失效会不会立即删除？
    - 不会，Redis 的过期删除策略是选择「惰性删除+定期删除」这两种策略配和使用。
      - 惰性删除策略的做法是，不主动删除过期键，每次从数据库访问 key 时，都检测 key 是否过期，如果过期则删除该 key。
      - 定期删除策略的做法是，每隔一段时间「随机」从数据库中取出一定数量的 key 进行检查，并删除其中的过期key。

  - 那为什么我不过期立即删除？
    - 在过期 key 比较多的情况下，删除过期 key 可能会占用相当一部分 CPU 时间，在内存不紧张但 CPU 时间紧张的情况下，将 CPU 时间用于删除和当前任务无关的过期键上，无疑会对服务器的响应时间和吞吐量造成影响。所以，定时删除策略对 CPU 不友好。

- Redis是怎么找到key存储在哪个节点上
  - Redis Cluster 将整个 key 空间划分为 16384 个槽（slot），每个主节点负责一部分槽（slot 范围），每个槽对应若干个KEY
  - Redis 对 key 的槽编号计算规则是：slot = CRC16(key) % 16384 得到槽号（0 ~ 16383）。
  - 客户端如何知道 key 在哪个节点？
    - 客户端连接 Redis 集群时，最初只知道一个节点。Redis 使用一种“跳转机制”来引导客户端找到正确节点：
      - 客户端向某个节点发送命令；
      - 若该节点不负责该 key 对应的槽，会返回 MOVED <slot> <target-ip:port>
      - 客户端收到后更新本地槽映射表（slot → 节点）直接重定向请求到正确节点。
  - 槽迁移
    - 当你添加或删除节点时，Redis 通过迁移槽（slot）实现数据再平衡
    - Redis 会将部分槽从 Node A 挪到新节点 Node D；
    - 在迁移期间：
      - 源节点仍接收请求，但可能返回 ASK；这表示：这个槽我正在迁移中，数据可能已经到另一台机器，请你这次请求去目标节点处理，但不要更新路由表！
      - 客户端会临时跳转到目标节点；
    - 迁移完成后，更新全局槽映射。


- String 是使用什么存储的?为什么不用 c 语言中的字符串?
  - Redis 的 String 字符串是用 SDS 数据结构存储的。
  - 结构中的每个成员变量分别介绍下：
    - len，记录了字符串长度。这样获取字符串长度的时候，只需要返回这个成员变量值就行，时间复杂度只需要 O（1）。
    - alloc，分配给字符数组的空间长度。这样在修改字符串的时候，可以通过 alloc - len 计算出剩余的空间大小，可以用来判断空间是否满足修改需求，如果不满足的话，就会自动将 SDS 的空间扩展至执行修改所需的大小，然后才执行实际的修改操作，所以使用 SDS 既不需要手动修改 SDS 的空间大小，也不会出现前面所说的缓冲区溢出的问题。
    - flags，用来表示不同类型的 SDS。一共设计了 5 种类型，分别是 sdshdr5、sdshdr8、sdshdr16、sdshdr32 和 sdshdr64，后面在说明区别之处。
    - buf[]，字符数组，用来保存实际数据。不仅可以保存字符串，也可以保存二进制数据。
  - 总的来说，Redis 的 SDS 结构在原本字符数组之上，增加了三个元数据：len、alloc、flags，用来解决 C 语言字符串的缺陷。
    - O（1）复杂度获取字符串长度
      - C 语言的字符串长度获取 strlen 函数，需要通过遍历的方式来统计字符串长度，时间复杂度是 O（N）。
      - 而 Redis 的 SDS 结构因为加入了 len 成员变量，那么获取字符串长度的时候，直接返回这个成员变量的值就行，所以复杂度只有 O（1）。
    - 二进制安全
      - 因为 SDS 不需要用 “\0” 字符来标识字符串结尾了，而是有个专门的 len 成员变量来记录长度，所以可存储包含 “\0” 的数据。但是 SDS 为了兼容部分 C 语言标准库的函数， SDS 字符串结尾还是会加上 “\0” 字符。
      - 因此， SDS 的 API 都是以处理二进制的方式来处理 SDS 存放在 buf[] 里的数据，程序不会对其中的数据做任何限制，数据写入的时候时什么样的，它被读取时就是什么样的。
      - 通过使用二进制安全的 SDS，而不是 C 字符串，使得 Redis 不仅可以保存文本数据，也可以保存任意格式的二进制数据。
    - 不会发生缓冲区溢出
      - C 语言的字符串标准库提供的字符串操作函数，大多数（比如 strcat 追加字符串函数）都是不安全的，因为这些函数把缓冲区大小是否满足操作需求的工作交由开发者来保证，程序内部并不会判断缓冲区大小是否足够用，当发生了缓冲区溢出就有可能造成程序异常结束。
      - 所以，Redis 的 SDS 结构里引入了 alloc 和 len 成员变量，这样 SDS API 通过 alloc - len 计算，可以算出剩余可用的空间大小，这样在对字符串做修改操作的时候，就可以由程序内部判断缓冲区大小是否足够用。
      - 而且，当判断出缓冲区大小不够用时，Redis 会自动将扩大 SDS 的空间大小，以满足修改所需的大小。

- Redis实现并发锁的方式
  - setnx
  - redisson
    - 可重入锁（同线程可多次加锁）
    - 自动续期机制（看门狗 Watchdog）
    - Lua 脚本保证原子性
  - redlock
    - 当系统是 多 Redis 实例（分布式 Redis 集群） 时，用 RedLock 算法确保高可靠性。
    - 实现思路
      - 在 N 个独立 Redis 实例 上同时尝试加锁（推荐 N=5）
      - 如果在 超过一半实例（N/2+1）加锁成功 且时间未超时，则认为加锁成功
      - 加锁失败则在所有实例上释放锁并重试
    - RedLock 的优势
      - 不依赖单点 Redis（一个实例挂了锁仍然安全）
      - 解决了单实例 Redis 在主从延迟或 failover 时的安全问题

- redis存登陆token 如果token被淘汰了怎么办 如果token泄漏了怎么办
  - 如果 token 被 Redis 淘汰了怎么办？
    - 使用独立 Redis 实例或数据库
    - 双层机制：Redis + JWT 自验证
      - 即使 Redis 挂了或丢失缓存，也能通过 JWT 自身验证继续生效。
    -  Redis 持久化
  - 如果 token 泄漏了怎么办？
    - 缩短 token 过期时间，被盗的 token 也只能短期生效。
    -  加入 Redis 校验机制（黑名单）当用户登出、改密、异常登录时，主动删除或加入黑名单：
    - 绑定客户端信息在 JWT payload 中嵌入部分信息：验证时比对 IP、UA 等信息不符则可以强制前一个 token 失效
    - 使用 HTTPS
    - Refresh Token 机制
      - 用户用 refresh token 换取新 access token；
      - 一旦泄漏，可立即让 refresh token 失效，彻底断开会话。
  



- TCP 的 TIME_WAIT 状态是怎么产生的，用什么命令可以查看当前有多少连接处于 TIME_WAIT 状态？
  - 在 TCP 四次挥手（连接关闭）过程中，主动发起关闭的一方（也就是先发出 FIN 的一方）最终会进入 TIME_WAIT 状态。
  - 四次挥手的过程
    - 客户端 → 服务端： 发送 FIN，表示我这边的数据发完了。
    - 服务端 → 客户端： 回复 ACK，确认收到你的 FIN。
    - 服务端 → 客户端： 当服务端也发送完数据后，发出自己的 FIN。
    - 客户端 → 服务端： 回复 ACK 确认收到。
  - 为什么要进入 TIME_WAIT？
    - 确保对方收到最后的 ACK（防止丢失导致连接状态不同步）；
    - 让旧连接的延迟包在网络中自然消失，防止新连接误收旧包。
    - 因此，TCP 规范（RFC 793）要求主动关闭方在进入 TIME_WAIT 后 保持 2MSL（Maximum Segment Lifetime）时间。MSL（报文最大生存时间）通常取 30 秒或 60 秒，所以 TIME_WAIT 一般持续 60~120 秒。
  - 查看当前有多少连接处于 TIME_WAIT 状态
    - netstat -an | grep TIME_WAIT | wc -l
    - ss -ant state time-wait | wc -l

- TCP Server 在网络通信时会涉及哪些系统调用？
  #image("Screenshot_20251015_145233.png")
  #image("Screenshot_20251015_145303.png")

- io多路复用 SELECT和EPOLL
  - #image("Screenshot_20251015_145638.png")
  - select 每次调用都要遍历整个 FD 集合，即使只有一个就绪。
  - epoll 由内核事件回调机制维护就绪队列，只返回活跃的 FD。

  - epoll底层 
    - 红黑树+就绪链表+等待队列
    - 所有监听 FD 存在红黑树中，增删改查为 O(logN)
    - 事件触发时直接放入 ready list，不需要扫描全部 FD
    - 回调机制触发由内核事件驱动而非轮询
    - 一次注册、持续监听

- -  TCP三次握手过程说一下？
  - 一开始，客户端和服务端都处于 CLOSE 状态。先是服务端主动监听某个端口，处于 LISTEN 状态
  - 客户端会随机初始化序号（client_isn），将此序号置于 TCP 首部的「序号」字段中，同时把 SYN 标志位置为 1，表示 SYN 报文。接着把第一个 SYN 报文发送给服务端，表示向服务端发起连接，该报文不包含应用层数据，之后客户端处于 SYN-SENT 状态。
  - 服务端收到客户端的 SYN 报文后，首先服务端也随机初始化自己的序号（server_isn），将此序号填入 TCP 首部的「序号」字段中，其次把 TCP 首部的「确认应答号」字段填入 client_isn + 1, 接着把 SYN 和 ACK 标志位置为 1。最后把该报文发给客户端，该报文也不包含应用层数据，之后服务端处于 SYN-RCVD 状态。
  - 客户端收到服务端报文后，还要向服务端回应最后一个应答报文，首先该应答报文 TCP 首部 ACK 标志位置为 1 ，其次「确认应答号」字段填入 server_isn + 1 ，最后把报文发送给服务端，这次报文可以携带客户到服务端的数据，之后客户端处于 ESTABLISHED 状态。
  - 服务端收到客户端的应答报文后，也进入 ESTABLISHED 状态。
  - 从上面的过程可以发现第三次握手是可以携带数据的，前两次握手是不可以携带数据的，这也是面试常问的题。
  - 一旦完成三次握手，双方都处于 ESTABLISHED 状态，此时连接就已建立完成，客户端和服务端就可以相互发送数据了。

- tcp为什么需要三次握手建立连接？
  - 三次握手的原因：
    - 三次握手才可以阻止重复历史连接的初始化（主要原因）
    - 三次握手才可以同步双方的初始序列号
    - 三次握手才可以避免资源浪费
  - 原因一：避免历史连接
    - 我们考虑一个场景，客户端先发送了 SYN（seq = 90）报文，然后客户端宕机了，而且这个 SYN 报文还被网络阻塞了，服务并没有收到，接着客户端重启后，又重新向服务端建立连接，发送了 SYN（seq = 100）报文（注意！不是重传 SYN，重传的 SYN 的序列号是一样的）。
    - 客户端连续发送多次 SYN（都是同一个四元组）建立连接的报文，在网络拥堵情况下：
      - 一个「旧 SYN 报文」比「最新的 SYN」 报文早到达了服务端，那么此时服务端就会回一个 SYN + ACK 报文给客户端，此报文中的确认号是 91（90+1）。
      - 客户端收到后，发现自己期望收到的确认号应该是 100 + 1，而不是 90 + 1，于是就会回 RST 报文。
      - 服务端收到 RST 报文后，就会释放连接。
      - 后续最新的 SYN 抵达了服务端后，客户端与服务端就可以正常的完成三次握手了。
    - 上述中的「旧 SYN 报文」称为历史连接，TCP 使用三次握手建立连接的最主要原因就是防止「历史连接」初始化了连接。
    - 如果是两次握手连接，就无法阻止历史连接，那为什么 TCP 两次握手为什么无法阻止历史连接呢？
    - 主要是因为在两次握手的情况下，服务端没有中间状态给客户端来阻止历史连接，导致服务端可能建立一个历史连接，造成资源浪费。
    - 你想想，在两次握手的情况下，服务端在收到 SYN 报文后，就进入 ESTABLISHED 状态，意味着这时可以给对方发送数据，但是客户端此时还没有进入 ESTABLISHED 状态，假设这次是历史连接，客户端判断到此次连接为历史连接，那么就会回 RST 报文来断开连接，而服务端在第一次握手的时候就进入 ESTABLISHED 状态，所以它可以发送数据的，但是它并不知道这个是历史连接，它只有在收到 RST 报文后，才会断开连接。
    - 可以看到，如果采用两次握手建立 TCP 连接的场景下，服务端在向客户端发送数据前，并没有阻止掉历史连接，导致服务端建立了一个历史连接，又白白发送了数据，妥妥地浪费了服务端的资源。
    - 因此，要解决这种现象，最好就是在服务端发送数据前，也就是建立连接之前，要阻止掉历史连接，这样就不会造成资源浪费，而要实现这个功能，就需要三次握手。
  - 原因二：同步双方初始序列号
    - TCP 协议的通信双方， 都必须维护一个「序列号」， 序列号是可靠传输的一个关键因素，它的作用：
      - 接收方可以去除重复的数据；
      - 接收方可以根据数据包的序列号按序接收；
      - 可以标识发送出去的数据包中， 哪些是已经被对方收到的（通过 ACK 报文中的序列号知道）；
    - 可见，序列号在 TCP 连接中占据着非常重要的作用，所以当客户端发送携带「初始序列号」的 SYN 报文的时候，需要服务端回一个 ACK 应答报文，表示客户端的 SYN 报文已被服务端成功接收，那当服务端发送「初始序列号」给客户端的时候，依然也要得到客户端的应答回应，这样一来一回，才能确保双方的初始序列号能被可靠的同步。
    - 四次握手与三次握手
      - 四次握手其实也能够可靠的同步双方的初始化序号，但由于第二步和第三步可以优化成一步，所以就成了「三次握手」。
      - 而两次握手只保证了一方的初始序列号能被对方成功接收，没办法保证双方的初始序列号都能被确认接收。
  - 原因三：避免资源浪费 
    - 如果只有「两次握手」，当客户端发生的 SYN 报文在网络中阻塞，客户端没有接收到 ACK 报文，就会重新发送 SYN ，由于没有第三次握手，服务端不清楚客户端是否收到了自己回复的 ACK 报文，所以服务端每收到一个 SYN 就只能先主动建立一个连接，这会造成什么情况呢？
    - 如果客户端发送的 SYN 报文在网络中阻塞了，重复发送多次 SYN 报文，那么服务端在收到请求后就会建立多个冗余的无效链接，造成不必要的资源浪费。
    - 即两次握手会造成消息滞留情况下，服务端重复接受无用的连接请求 SYN 报文，而造成重复分配资源。


- 登陆方案
  - 单 token + JWT + Redis（可选黑名单/设备绑定）
  - 双 token（Access + Refresh）
  - 双 token + 单点登录系统（SSO） + OAuth2 / OpenID Connect


- Springboot相比较spring好处
  - 先看 Spring 有哪些“痛点”
    - 配置繁琐（XML 地狱）
    - 依赖管理复杂
      - 版本冲突等
    - 环境部署麻烦
      - 传统 Spring Web 应用需要：打成 .war 部署到 Tomcat 或其他外部容器中
  - Springboot相比较spring好处
    - 自动配置
      - 不再需要写大量 XML 或 \@Configuration； 
      - 通过 “约定大于配置” 的理念，自动为你配置好常用组件。
    - 起步依赖
      - starter 依赖
    - 内嵌服务器
      - 无需外部部署 WAR 包
    - 健康检查与监控
    - 一致的配置方式（application.yml / .properties）
    - 与微服务体系兼容良好


- MySQL事务隔离级别？分别解决什么问题？
  - 读未提交
  - 读可提交
    - 解决脏读问题
      - 每次select生成快照
      - 脏读：读到事务未提交的数据
  - 可重复读
    - 解决脏读和不可重复读问题
      - sql标准：锁当前行，读数据加共享锁，写数据加排他锁
      - 不可重复读：两次读到的数据不一样
  - 可串行化
    - 强制加锁，串行执行
    - 解决幻读问题
      - 幻读：两次读到的数据条数不一样

- sql标准的rr和innodb的rr不一样
  - SQL 标准的 RR 只保证“已读行不变”，但不锁定查询范围，所以其他事务可以插入新行导致“幻读”。RR 在标准里 ≠ “事务级快照”，而是 “行级锁定可重复读”。
  - InnoDB 的 RR 加了 MVCC + 间隙锁，既能保持行内容一致，又能锁定范围，从而防止幻读。


- 三类存储引擎分别支持哪些索引？
  #image("Screenshot_20251018_193137.png")

- 不同存储引擎的优缺点？
  - InnoDB
    - 优点：
      - 支持 事务（ACID），保证数据一致性。
      - 行级锁，高并发写入性能好。
      - 支持 外键约束，保证参照完整性。
      - 默认采用 聚簇索引，查询主键非常快。
      - 支持崩溃恢复（redo log、undo log）。
    - 缺点：
      - 相比 MyISAM，占用空间大。
      - 全文索引支持起步晚（MySQL 5.6+）。
      - 写入稍慢（事务、行锁、日志开销）。
  -  MyISAM
    - 优点
      - 存储格式简单，查询速度快，尤其是 读密集型场景。
      - 全文索引支持早，适合全文搜索。
      - 占用空间比 InnoDB 小。
    - Memory
      - 优点：
        - 数据 全部存储在内存，读写速度极快。
        - 支持 表级锁、哈希索引，精确查找速度快。
      - 缺点：
        - 数据 断电或重启丢失，不持久化。
        - 哈希索引只适合精确匹配，范围查询性能差。
        - 表大小受内存限制，通常用于 临时表或缓存。

- 聚簇索引和非聚簇索引
  - 聚簇索引：通常是 B+ 树叶子节点存储实际数据行。查询主键数据直接命中叶子节点，无需再回表。
  - 非聚簇索引：B+ 树叶子节点只存 索引列 + 指向数据行的地址（InnoDB 是主键，MyISAM 是物理行地址）。查询非主键列时，需要先查索引，再根据 rowid 回表取数据。