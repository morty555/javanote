= 场景题
- 设计一个广告计费系统，用户曝光点击会产生计费，有每日限额，如何设计，可能遇到什么问题及如何解决？最后一块钱问题怎么解决？
  - 系统要求
    - 曝光和点击触发扣费事件
    - 限额规则
      - 每个广告主每日预算
      - 系统必须保证不超支
    - 高并发要求
      - 每天可能亿级曝光和百万级点击
      - 计费需要实时或者准实时
    - 精度要求
      - 每一笔计费必须精确，最后一块钱不能丢
    - 每日重置
  - 系统设计 
    - 收集曝光/点击事件 → 发送到 消息队列
    - 消费消息队列 → 更新广告消耗
    - 支持每日限额检查
    - 实时计费层（Redis/内存缓存计数）保证快速读取和更新
    - 落地计费层（MySQL / ClickHouse / HBase）存储最终计费记录和统计
    - Redis和MYSQL的同步可以异步
    - Redis counter 每日 0 点重置
  - 问题和解决方案
    - 高并发计数导致超支
      - 使用 Redis 原子操作（HINCRBYFLOAT / Lua 脚本）保证限额不被超越
    - 分布式并发导致超支
      - 全局计数使用 Redis 分布式锁 / 原子脚本
    - 数据落地延迟
      - 批量写数据库，并异步补偿，保证账目最终一致
    - 精度问题（浮点误差）
      - 使用 整数计费（分为最小货币单位，如分） 避免浮点精度问题
    - 宕机导致丢数据
      - 消息队列保证 事件至少一次投递，落地时支持 幂等
  - 最后一块钱问题
    - 当广告剩余预算不足以覆盖一次曝光或点击时，如何处理？
      - 不投放策略
      - 按比例扣费
  

- 秒杀优惠券的核心需求
  - 高并发发放：短时间内可能有成千上万用户请求。
  - 库存控制：优惠券数量有限，不能超发。
  - 防刷机制：防止刷单、恶意抢券。
  - 实时性：用户要尽快知道抢到还是抢不到。
  - 可扩展性：在用户量大时系统仍能稳定。
  - 秒杀抢券的常见策略
    - 方案 A：数据库行锁 (悲观锁)
      - 优点：简单，保证不超发。
      - 缺点：高并发下会成为数据库瓶颈，容易锁表或锁行。
    - 方案 B：乐观锁 + 版本号
      - 优点：减少锁等待，性能较悲观锁好。
      - 缺点：高并发下仍可能有大量重试。
    - 方案 C：Redis + 原子操作（推荐）
  - 防刷策略
    - 用户限额：每个用户只能领取一次。
    - 验证码 / 登录限制：防止机器人。
    - IP限流：每秒同一IP请求次数限制。
    - 秒杀队列：通过消息队列限流（RabbitMQ、Kafka）。
  - 异步处理+消息队列
    - 高并发时，前端请求可以先进入 消息队列，后台逐个消费写入数据库：
    - 避免数据库瞬间爆炸。
    - 可以做抢券成功后异步发放通知。  

- 秒杀系统设计
  - 秒杀链接安全防刷
    - 动态生成秒杀链接，防止暴力破解
    - 验证码机制，防止机器人刷单
    - 用户登录限制，每个用户只能参与一次
  - 限流策略
    - 使用漏桶算法或令牌桶算法进行限流
    - 基于用户 ID 或 IP 地址进行限流
    - 使用分布式限流组件（如 Redis + Lua 脚本）
  - 幂等控制
    - Redis 记录用户是否已成功下单：SETNX(userId:skuId, 1)
    - 若 key 已存在 → 直接返回秒杀成功或拒绝
  - Redis 扣减库存（防超卖）
    ```
        -- Lua 脚本保证原子性
    local stock = redis.call("GET", KEYS[1])
    if (tonumber(stock) <= 0) then
        return 0
    else
        redis.call("DECR", KEYS[1])
        return 1
    end

    ```
  - 异步下单（削峰）
    - 扣完 Redis 库存后，将下单请求发送到消息队列
    - 后台异步消费消息队列，进行实际下单操作
  - 数据库防超卖（乐观锁）
    ```
    UPDATE product 
    SET stock = stock - 1 
    WHERE product_id = #{id} AND stock > 0;

    ```
  #image("Screenshot_20251101_152150.png")
  - 秒杀成功要到数据库写入成功后才返回


- 服务宕机的情况导致锁无法有效的释放，在生产环境怎么解决这个问题
  - redis
    - 设置过期时间（TTL）
    - 自动续期机制（Redisson 的做法）
      - 如果客户端宕机，续期任务中断，锁自然过期释放。
    - 锁标识验证（防止误删）
  - Zookeeper 分布式锁
    - 宕机或断连时，ZK 自动删除节点；
    - 性能较 Redis 弱；



- 如何来设计实现一个短链接系统
  - 生成短链接
    - 用户输入长 URL，返回一个短 URL。
  - 访问短链接
    - 用户访问短链接，系统重定向到原始长 URL。
  - 短链接生成策略
    - 自增 ID + 编码
      - 每条 URL 对应一个自增整数 ID
      - 将 ID 转为短字符串（通常 62 进制 [a-zA-Z0-9]）
      - 优点：
        - 简单、生成速度快
        - 可顺序增加，方便统计
      - 缺点：
        - 可预测性强
    - 哈希映射
      - 对长 URL 做 MD5 或 SHA256 哈希，再取前 6~8 位作为短码
      - 可防止顺序预测
      - 需要处理哈希冲突（可加随机后缀或重试）
    - 随机生成短码
      - 每次随机生成长度固定的字符串
      - 若已存在，重试
      - 缺点：可能冲突、需要检查数据库唯一性
      - 优点：不可预测
  - 数据库设计
    - short_code 唯一索引 → 快速查找短链接对应长 URL
  - 单机版本
    - 数据库存储映射关系
    - API Server 提供生成短链接和访问短链接接口
  - 高并发分布式版本
    - 缓存短链接映射：访问频繁的短链接先存 Redis，避免频繁访问 DB。
    - 分库分表：根据 ID 或哈希进行水平拆分。
    - 消息队列统计：点击量可以异步写入，避免请求阻塞。
    - 限流


- 如果要做一个抢单系统，你会怎么用 Redisson 来实现？
  - 业务场景分析
    - 同一个订单只能被一个司机/用户成功抢到。
    - 在高并发情况下，要防止：
      - 多个线程同时判断库存（或状态）为“可抢”；
      - 多人同时修改数据库，出现超卖 / 重复抢单。
    - 需要分布式锁（跨节点同步），不能靠 JVM 锁。
  - 核心实现思路
    - 为每个订单加分布式锁
      - 如果业务更看重抢单顺序，用RFairLock
      - 若更看重执行效率，用RLock
      #image("Screenshot_20251023_114033.png")
    -  防止重复抢（同一用户）
      - 如果要防止一个用户抢多个订单，可配合 RSet或 Lua 
    - 限流 
      - RRateLimiter 控制抢单接口 QPS
    

- 订单系统超时未支付如何自动关闭?
  - 消息队列延迟消息
    - 订单创建时，发送一条延迟消息到 MQ（如 RabbitMQ、Kafka、RocketMQ）
    - 延迟时间设置为支付截止时间（如 15 分钟）
    - 消息到期后，消费者接收到消息，检查订单状态
    - 如果订单仍未支付，则执行关闭操作并释放库存
    - 消息丢失
      - 生产者发送未确认
      - MQ 宕机未持久化
      - 消费者异常未 ack
      - 业务执行异常未重试
    - 解决方案
      - 生产者发送确认机制
      - MQ 持久化配置
      - 消费者幂等设计和异常重试机制
      - 数据层兜底：本地消息表 + 定时补偿任务
  - 定时任务扫描
    - 缺点：不够实时（最多延迟5分钟），高并发场景下会扫描压力大
  - Redis Key TTL 监听
    - 缺点：过期事件不是 100% 可靠，Redis 重启或丢失通知会漏单。

- 如何设计一个分布式ID生成器?
  - 目标与约束
    - 唯一性：全局唯一（必须）。
    - 可排序性：是否需要按生成时间顺序可比较/有序？
    - 吞吐量：每秒/每毫秒需多少 ID？
    - 短/长：是否需要短字符串（URL/前端）还是可以用 64-bit 数字？
    - 无单点/高可用：是否允许中心化组件（DB/Redis）？
    - 幂等/幂等校验：是否需要重复检测？
    - 容错（时钟回退）：如何应对机器时钟后退？
    - 可解析：是否希望 ID 中包含时间、机器信息便于追踪？
  - 常见方案
    - Twitter Snowflake
      - 优点：单机性能高（数万 / 秒），64-bit 整数、按时间递增（大致有序）。
      - 缺点：需要合理分配 workerId，时钟回退处理需设计。
      - 经典 64-bit Snowflake 布局（常见实现）：
        - 1 bit 符号位（始终为0）
        - 41 bits 时间戳（毫秒级，约69年）
        - 10 bits 机器ID（1024台机器）
        - 12 bits 序列号（每毫秒最多4096个ID）
      - 时钟回退解决方案
        - 等待时间回退结束
        - 使用备用序列区（逻辑时钟）
        - 使用逻辑单调时间（单调时钟）
    - 数据库自增序列（主库/分片）
      - 优点：实现简单、强一致。
      - 缺点：单点瓶颈、扩展性差、高并发下性能不足。
    - Redis INCR / Lua 脚本
      - 优点：性能高、实现简单。
      - 缺点：单点瓶颈（可用 Redis Cluster 增强。）、需要持久化防数据丢失。
    - UUID / UUIDv1 / ULID / KSUID
      - UUIDv4 随机：无需协调，但长、不按时间排序。
      - ULID/KSUID：时间有序并且是字符串形式，适合需要可读性和排序的场景。
    - Zookeeper 顺序节点
      - 优点：强一致、分布式协调。自动排序。
      - 缺点：性能较低，复杂度高。

- 如何设计一个高性能的排行榜?
  - 使用 Redis Sorted Set（ZSet）作为核心结构
    - score：存玩家分数
    - member：存玩家 ID
    - 支持按 score 排序、范围查询、倒序排名等操作
    - 操作复杂度为 O(log n)
  - 更新分数：
    ```
    ZADD leaderboard score user_id
    ```
    - O(log n)
  - 获取前N
    ```
    ZREVRANGE leaderboard 0 N-1 WITHSCORES
    ```
    - O(log n + m)，m为返回元素数量
  - 获取用户排名
    ```
    ZREVRANK leaderboard user_id
    ```
    - O(log n)
  - 获取用户分数
    ```
    ZSCORE leaderboard user_id
    ```
    - 	O(1)
  - 获取某范围分数段
    ```
    	ZREVRANGEBYSCORE leaderboard max min LIMIT offset count
    ```
    - O(log n + m)



- 一个用户下两次单怎么办
  - 正常场景：允许多次下单
    - 比如淘宝、外卖、订票——用户确实可以多次下单。
    - 允许重复下单，只要每次订单号唯一；
    - 保证支付幂等性（防止重复支付）；
    - 订单状态分离：已下单 / 已支付 / 已取消。
  - 业务上不允许重复下单（如秒杀/活动/报名）
    - 此时要防止同一用户重复下单。
    - 接口幂等性控制
      - 每个用户操作生成一个唯一“幂等键”（例如 userId + activityId），
      -  数据库唯一约束（强一致）
      - 分布式锁
      - 消息队列异步下单
        - 消费端处理时判断用户是否已下单；
  - 防止“支付重复提交”
    - 即便订单只生成一次，用户可能点击支付多次。
    - 生成唯一 paymentId；
    - 支付接口幂等控制；
    - 支付回调通过 trade_no 唯一更新订单状态；
- 一个用户下一次单立刻退出并进入再下两次单怎么办（想买两件商品）
  - 每次下单 → 新建一个订单（订单号唯一）
  - 这样即使用户退出再进、下几次单，每次订单号都不同，没问题。
  - 关键是要防止他在同一次购买流程里重复点下单按钮。
  - 常见的处理方式
    - 下单按钮点击后立即置灰；
    - 接口响应后再恢复；
    - 每次进入下单页重新生成 token。
    - 前端提交时带上 orderToken；
    - Redis 保存 token
    - 创建成功后删除 token。
  - 支持多件购买的正确做法
    - 用户在购物车勾选多件商品 → 提交一次订单。
    - 每件商品都可单独下单，后端逻辑中不要对 user_id 做唯一约束，而只做活动级别或订单 token 限制。


- 如果系统每秒最多处理10个请求，但秒杀瞬间来了300个请求，你会如何设计系统来应对？
  - 流量削峰填谷
  - 排队异步处理
  - 预减库存 / 内存缓存

- 在这种高并发秒杀场景下，如何避免超卖或少卖的问题？
  - Redis 原子扣减库存
  - 数据库乐观锁 / CAS
  - 队列异步扣库存
  - 令牌（Token）机制
  - 少卖问题场景
    - 队列处理延迟
      - 如果秒杀活动时间结束或队列消费者未及时消费，队列剩余请求可能被丢弃或超时。
    - 异步落库失败
      - Redis 或内存做预扣减库存，异步写数据库。异步写入数据库失败，或者订单创建失败未回滚库存。
  - 防少卖策略
    - 库存回补：异步处理失败订单时，将库存回滚到 Redis。
    - 高峰时增加队列消费者，减少库存未卖掉的情况。

- 如果要存50万高考成绩，用什么索引最好
  - id 或 student_id 可以建主键索引。
  - 普通 B-Tree 索引
  - 联合索引顺序很重要，应该把选择性高的字段放前面
  - 覆盖索引
  - 


