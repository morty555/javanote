- ARMS系统监控
  - 阿里云
  - 探针技术 javaagent 在java应用启动时或运行时动态修改字节码 无侵入式监控
  - prometheus+Grafana
    - prometheus
      - 时序数据库 拉取模式 
      - 指标类型
      - 内存 预写日志 持久化到硬盘
    - Grafana
      - 数据可视化
- mongodb
  - 文件和集合
  - BSON
  - 数据页，页号，层级
  - 写时复制，b+树
  - cache
  - 写前日志，顺序写入（磁盘随机写入）
  - checkpoint
  - wiredtiger存储引擎
  - server层
    - 解析器
    - 优化器
    - 执行器
  - 路由mongos。，分片
  - 副本集，类似于主从
  - 和mysql对比
  #image("Screenshot_20250823_101851.png")
  #image("Screenshot_20250823_101923.png")
  #image("Screenshot_20250823_101931.png")
- DSTransactional注解
  - 多数据源分布式事务
  - 应用于一个方法同时操作两个库
  - 如果是 跨服务的分布式事务，则需要引入 Seata、RocketMQ 事务消息 等方案，DSTransactional 只适合 单应用多数据源 的场景。
- xxl-job
    - springboot的定时注解在多节点部署时会重复执行，如定时积分增加，多节点就会增加两次
    - 使用xxl-job可以解决这个问题
  - 分布式平台的作用
    - 分布式调度平台可以保证高可用
    - 防止重复执行
    - 突破单机处理极限
  - #image("Screenshot_20250915_224617.png")
  - 执行器
    - 配置时需要配置job类和config类
    - 要执行job类的代码，需要到调度中心去手动执行
      - 配置新增任务
      - 选择执行器执行
    - bean模式
      - job类实现加入xxljob注解，调度中心指定名称，会找到执行器中同名方法调用
    - glue模式
      - GlueIDE编辑代码指定我们要调用执行器中的哪个方法
    - 需要注意多机部署下的执行器端口号要不同
    - 多机部署可选择负载均衡策略
    - 分片广播
      - 会给执行器传递分配总数和分配索引
      - 如果有2000条记录，可以根据id取模分片总数得到的序号和分配索引进行匹配
      - 如果分配总数是2,索引是0，那么id%2=0的记录就会被这个执行器处理。
      - 这样就能提高效率，并且效率随着分片总数增加而增加
      #image("Screenshot_20250915_231454.png")
      #image("Screenshot_20250915_231507.png")
- minio
  - 适合公司或个人想要自建私有云存储，自己管数据。
  - 自建的对象存储服务
  #image("Screenshot_20250915_232223.png")
  - MinIO 使用纠删码机制来保证高可靠性，使用 highwayhash 来处理数据损坏
  - 关于纠删码，简单来说就是可以通过数学计算，把丢失的数据进行还原，它可以将n份原始数据，增加m份数据，并能通过n+m份中的任意n份数据，还原为原始数据。即如果有任意小于等于m份的数据失效，仍然能通过剩下的数据还原出来。
  - 文件对象上传到 MinIO ，会在对应的数据存储磁盘中
  - 单机部署
    - minio server的standalone模式，即要管理的磁盘都在host本地。该启动模式一般仅用于实验环境、测试环境的验证和学习使用。在standalone模式下，还可以分为non-erasure code mode和erasure code mode。
      - non-erasure code mode
        - 在此启动模式下，对于每一份对象数据，minio直接在data下面存储这份数据，不会建立副本，也不会启用纠删码机制。因此，这种模式无论是服务实例还是磁盘都是“单点”，无任何高可用保障，磁盘损坏就表示数据丢失。
      - erasure code mode
        - 此模式为minio server实例传入多个本地磁盘参数。一旦遇到多于一个磁盘参数，minio server会自动启用erasure code mode。erasure code对磁盘的个数是有要求的，如不满足要求，实例启动将失败。 erasure code启用后，要求传给minio server的endpoint(standalone模式下，即本地磁盘上的目录)至少为4个。
  - 分布式集群部署
    - 分布式 Minio 可以让你将多块硬盘（甚至在不同的机器上）组成一个对象存储服务。由于硬盘分布在不同的节点上，分布式Minio避免了单点故障。
    - 分布式存储，很关键的点在于数据的可靠性，即保证数据的完整，不丢失，不损坏。只有在可靠性实现的前提下，才有了追求一致性、高可用、高性能的基础。而对于在存储领域，一般对于保证数据可靠性的方法主要有两类，一类是冗余法，一类是校验法。
      - 冗余法
        - 冗余法最简单直接，即对存储的数据进行副本备份，当数据出现丢失，损坏，即可使用备份内容进行恢复，而副本备份的多少，决定了数据可靠性的高低。这其中会有成本的考量，副本数据越多，数据越可靠，但需要的设备就越多，成本就越高。可靠性是允许丢失其中一份数据。当前已有很多分布式系统是采用此种方式实现，如 Hadoop 的文件系统（3个副本），Redis 的集群，MySQL 的主备模式等。
      - 校验法
        - 校验法即通过校验码的数学计算的方式，对出现丢失、损坏的数据进行校验、还原。注意，这里有两个作用，一个校验，通过对数据进行校验和( checksum )进行计算，可以检查数据是否完整，有无损坏或更改，在数据传输和保存时经常用到，如 TCP 协议；二是恢复还原，通过对数据结合校验码，通过数学计算，还原丢失或损坏的数据，可以在保证数据可靠的前提下，降低冗余，如单机硬盘存储中的 RAID技术，纠删码（Erasure Code）技术等。MinIO 采用的就是纠删码技术。
    - 分布式Minio优势
      - 数据保护
        - 分布式Minio采用 纠删码来防范多个节点宕机和位衰减 bit rot 。
        - 分布式Minio至少需要4个硬盘，使用分布式Minio自动引入了纠删码功能。
      - 高可用
        - 单机Minio服务存在单点故障，相反，如果是一个有N块硬盘的分布式Minio,只要有N/2硬盘在线，你的数据就是安全（读）的。不过你需要至少有N/2+1个硬盘来创建（写）新的对象。
        - 例如，一个16节点的Minio集群，每个节点16块硬盘，就算8台服務器宕机，这个集群仍然是可读的，不过你需要9台服務器才能写数据。
        - 写数据要大于一半，是为了避免脑裂问题。
          - 脑裂问题是各节点都认为自己是主节点，导致数据不一致。
          - 写数据要求硬盘大于一半就导致一个集群只能有一个主节点，避免脑裂。
      - 一致性
        - Minio在分布式和单机模式下，所有读写操作都严格遵守read-after-write一致性模型。
- Caffeine 
  - java本地缓存，效率比redis高（省去网络IO），占用本机或服务器内存
  - Caffeine和redis双缓存
    - caffeine快，但是没有备份，重启丢数据，而且容量有限，多节点不共享，在多机部署有一致性问题
    - redis跨节点共享，数据持久化
  - 双缓存的作用
    - 提升性能，降低延迟
      - 请求数据时，先查 Caffeine（本地缓存），命中直接返回，几乎无开销。
      - 本地没命中时，再去查 Redis。
      - Redis 也没命中，最后才查数据库。
      - 这样大大减少数据库压力，Redis 也不会被高频访问打爆。
    - 减少 Redis 压力
      - 频繁被访问的热点数据会被保存在每个应用节点的 Caffeine 中。
      - 应用节点优先命中本地缓存，只有本地没命中时才访问 Redis。
      - Redis 的请求量降低，整个系统更稳定。
    - 解决分布式环境下缓存一致性
      - 单纯 Caffeine 没法跨节点共享。
      - 引入 Redis 作为“权威缓存”，Caffeine 作为“近端缓存”。
      - 当数据更新时，Redis 作为源头，可以通过消息队列、订阅（pub/sub）或删除通知的方式，让各节点 Caffeine 失效或更新，保证一致性。
    - 避免缓存穿透、击穿、雪崩问题
      - Redis 作为分布式缓存层，保证在 Caffeine 没命中时能抗住大部分流量。
      - Caffeine 可以缓存空值、短期热点，缓冲 Redis 瞬时压力。
      - 两者结合，分摊压力，降低大流量冲击时的风险。
    #image("Screenshot_20250917_000632.png")
  - 存在的问题
    - 数据一致性
      - Redis 和 Caffeine 数据不同步，本地缓存里是旧数据，Redis 已经更新。
      - 应对
        - 更新数据时同时更新 Redis，然后通知各节点清理 Caffeine（常见做法是用消息队列、Redis Pub/Sub、Spring Cache 的 CacheEvict）。
        - Caffeine 设置 较短 TTL，过期后会自动去 Redis 取最新的。
    - 缓存更新/失效策略复杂
      - 一个缓存失效后，另一个可能还保留旧数据。
      - 应对：
        - 需要保证 两级缓存一致地失效，否则可能出现“Caffeine 旧数据覆盖 Redis 新数据”的情况。
        - 不在 Caffeine 里做写操作（只缓存 Redis 读到的数据）。更新数据时只更新 Redis，再通知 Caffeine 清理，让 Caffeine 重新从 Redis 拉。
    - 内存占用 & JVM GC 压力
      - Caffeine 缓存是存在 JVM 内的，数据太多会导致内存爆炸，引发频繁 GC，甚至 OOM。
      - 应对：  
        - Caffeine 只存热点数据，容量要限制（maximumSize 或 expireAfterAccess）。
        - 大容量、全量数据不要放本地缓存。
    - 分布式环境下的同步问题
      - 多个应用节点，每个节点都有一份 Caffeine，更新时如何保证所有节点都失效？
      - 应对：
        - 使用 Redis 发布订阅（Pub/Sub）通知各节点清除 Caffeine。
        - 或者用 MQ（Kafka、RocketMQ 等）广播失效消息。
    - 缓存预热 & 穿透/击穿/雪崩
      - 服务刚启动时，Caffeine 还没数据，Redis 压力陡增。
      - 高并发场景下，热点数据同时过期可能导致 Redis 被打爆，甚至穿透到数据库
      - 应对：  
        - 服务启动时预热 Caffeine 缓存（比如批量加载一些热点数据）。
        - Redis 层加布隆过滤器/互斥锁防止击穿。
        - 设置随机过期时间，避免同时失效。
    - 在失效策略时强调二级缓存一致失效，在缓存穿透又要求避免同时失效，矛盾吗
      - 对于强一致性要求的数据
        - 如：库存、余额、订单状态、权限信息。
        - 如果 Redis 失效了，Caffeine 还保留旧数据，就可能导致下单超卖、权限绕过，后果严重。
        - 这类数据更新时，必须让 Redis 和 Caffeine 同时过期/删除，保证读取到的一定是最新值。
        - 策略：
          - 写操作时直接删除 Redis 和 Caffeine。
          - 读操作时重新从数据库/Redis 加载。
          - 这时 一致性优先，牺牲部分性能。
      - 读多写少、强一致性要求不高的数据
        - 如：商品详情页、热门新闻、推荐列表。
        - 如果所有缓存同时过期，高并发场景下会导致海量请求同时打到 Redis 或数据库，引发“缓存雪崩”。
        - 这些数据即使延迟几秒更新也没关系。
        - 策略：
          - 设置 随机 TTL（带一点抖动），避免同一时间大规模同时过期。
          - Caffeine 可以设置短 TTL（比如 30 秒），Redis 设置稍长 TTL（比如 5 分钟 + 随机 30 秒）。
          - 这样即使 Redis 大量 Key 同时过期，Caffeine 还能兜一阵子。      


- Sa-token
- snailjob

